{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb053a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "unique_terms = set()\n",
    "document_names = []\n",
    "word_dictionary = []\n",
    "query_dictionary = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de51667",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96da9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(lines, dictionary):\n",
    "    tokens = word_tokenize(lines)\n",
    "    tokens = [t.lower() for t in tokens]  # add the lower-case tokens to the set\n",
    "\n",
    "    count_docs(tokens, dictionary)\n",
    "    stemmed_tokens = [PorterStemmer().stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72528fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_docs(terms, dictionary):\n",
    "    unique_terms.update(terms)\n",
    "    document_dictionary = dict(Counter(terms))\n",
    "    dictionary.append(document_dictionary)\n",
    "    # for word in terms:\n",
    "    #     word_dictionary[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655461f",
   "metadata": {},
   "source": [
    "## Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c861c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(f'docs/{file}', 'r') as f:\n",
    "        lines = f.read()\n",
    "        document_names.append(file[:-4])\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76637e75",
   "metadata": {},
   "source": [
    "# Building Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fcec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_positional_index(path):\n",
    "    # Initialize the positional index\n",
    "    positional_index = {}\n",
    "    for doc_id, file in enumerate(os.listdir(path)):\n",
    "        lines = read_file(file)\n",
    "        terms = preprocessing(lines, word_dictionary)\n",
    "\n",
    "        for i, token in enumerate(terms):\n",
    "            # create a new posting list if it isn't there yet\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = [0, {}]\n",
    "\n",
    "            # get the existing posting list of the term, ex: {0: [0]} for the term {'comput': {0: [0]}}\n",
    "            term_list = positional_index.get(token, {})\n",
    "            # get the existing position list if that term appeared in a document, ex: [] if the doc_id is 1\n",
    "            term_positions = term_list[1].get(doc_id, [])\n",
    "\n",
    "            # append the new position in that list, ex: it becomes 1: [0]\n",
    "            positional_index[token][0] += 1\n",
    "            positional_index[token][1][doc_id] = term_positions + [i]\n",
    "\n",
    "    print(positional_index)\n",
    "    normalized_doc_df = tf(word_dictionary, document_names)\n",
    "    return positional_index, normalized_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfb32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4488ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_query(q, positional_index):\n",
    "    matches = [[] for i in range(10)]\n",
    "    q = preprocessing(q, query_dictionary)\n",
    "    for term in q:\n",
    "        print(term)\n",
    "        if term in positional_index.keys():\n",
    "            for key in positional_index[term][1].keys():\n",
    "                # print(key)\n",
    "                # print(positional_index[term][1])\n",
    "                if matches[key]:\n",
    "                    print(matches)\n",
    "                    print(positional_index[term][1][key][0])\n",
    "                    if matches[key][-1] == positional_index[term][1][key][0] - 1:\n",
    "                        print(matches[key])\n",
    "                        matches[key].append(positional_index[term][1][key][0])\n",
    "                else:\n",
    "                    matches[key].append(positional_index[term][1][key][0])\n",
    "                print(matches)\n",
    "\n",
    "    matched_docs = []\n",
    "    for pos, list in enumerate(matches, start=0):\n",
    "        if len(list) == len(q):\n",
    "            matched_docs.append(document_names[pos])\n",
    "\n",
    "    query_df = tf(query_dictionary)\n",
    "    return matched_docs, query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7b3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(terms_dictionary, index=[\"tf-raw\"]):\n",
    "    print(terms_dictionary, index)\n",
    "    term_frequency_df = pd.DataFrame(terms_dictionary, index=index)\n",
    "    term_frequency_df.fillna(0, inplace=True)\n",
    "    term_frequency_df = term_frequency_df.transpose()\n",
    "    print(term_frequency_df)\n",
    "    #\n",
    "    weighted_tf_df = term_frequency_df.applymap(weighted)\n",
    "    print(weighted_tf_df)\n",
    "\n",
    "    doc_frequency = term_frequency_df.sum(axis=1)\n",
    "    idf = len(document_names) / doc_frequency\n",
    "\n",
    "    inverse_doc_freq = pd.concat([doc_frequency, idf], axis=1)\n",
    "    inverse_doc_freq.columns = ['df', 'idf']\n",
    "    print(inverse_doc_freq)\n",
    "\n",
    "    tf_idf = term_frequency_df.multiply(idf, axis=0)\n",
    "    print(tf_idf)\n",
    "\n",
    "    doc_length = np.sqrt((tf_idf ** 2).sum())\n",
    "    print(doc_length)\n",
    "\n",
    "    normalized_tf_idf = tf_idf / doc_length\n",
    "    print(normalized_tf_idf)\n",
    "\n",
    "    if index[0] == 'tf-raw':\n",
    "        query_df = pd.concat([term_frequency_df, weighted_tf_df, idf, term_frequency_df, normalized_tf_idf], axis=1)\n",
    "        query_df.columns = ['tf-raw', 'weighted-tf', 'idf', 'tf-idf', 'normalized']\n",
    "        print(\"query length:\", doc_length)\n",
    "        print(query_df)\n",
    "        return query_df\n",
    "\n",
    "    return normalized_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89df3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(query_df, normalized_tf_idf, matched_docs):\n",
    "    query_terms = query_df.index\n",
    "    matched_docs_df = normalized_tf_idf.loc[query_terms, matched_docs]\n",
    "    print(matched_docs_df)\n",
    "\n",
    "    query_normalized = query_df.loc[:, 'normalized']\n",
    "    product_df = matched_docs_df.multiply(query_normalized, axis=0)\n",
    "    print(product_df)\n",
    "\n",
    "    similarity_score = product_df.sum().sort_values(ascending=False)\n",
    "    print(similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461abce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted(x):\n",
    "    if x > 0:\n",
    "        return np.log10(x) + 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b171b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}]\n",
      "['1', '10']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}]\n",
      "['1', '10', '2']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}]\n",
      "['1', '10', '2', '3']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}]\n",
      "['1', '10', '2', '3', '4']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}]\n",
      "['1', '10', '2', '3', '4', '5']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}, {'caeser': 1, 'mercy': 1, 'worser': 1}]\n",
      "['1', '10', '2', '3', '4', '5', '6']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}, {'caeser': 1, 'mercy': 1, 'worser': 1}, {'antony': 1, 'caeser': 1, 'mercy': 1}]\n",
      "['1', '10', '2', '3', '4', '5', '6', '7']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}, {'caeser': 1, 'mercy': 1, 'worser': 1}, {'antony': 1, 'caeser': 1, 'mercy': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}]\n",
      "['1', '10', '2', '3', '4', '5', '6', '7', '8']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}, {'caeser': 1, 'mercy': 1, 'worser': 1}, {'antony': 1, 'caeser': 1, 'mercy': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}]\n",
      "['1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}, {'caeser': 1, 'mercy': 1, 'worser': 1}, {'antony': 1, 'caeser': 1, 'mercy': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'angels': 1, 'fools': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}]\n",
      "{'antoni': [3, {0: [0], 2: [0], 6: [0]}], 'brutu': [3, {0: [1], 2: [1], 4: [0]}], 'caeser': [5, {0: [2], 2: [2], 4: [1], 5: [0], 6: [1]}], 'cleopatra': [1, {0: [3]}], 'merci': [5, {0: [4], 3: [0], 4: [2], 5: [1], 6: [2]}], 'worser': [4, {0: [5], 3: [1], 4: [3], 5: [2]}], 'fool': [4, {1: [0], 7: [1], 8: [1], 9: [1]}], 'fear': [3, {1: [1], 7: [2], 8: [2]}], 'in': [4, {1: [2], 7: [3], 8: [3], 9: [2]}], 'rush': [4, {1: [3], 7: [4], 8: [4], 9: [3]}], 'to': [4, {1: [4], 7: [5], 8: [5], 9: [4]}], 'tread': [4, {1: [5], 7: [6], 8: [6], 9: [5]}], 'where': [4, {1: [6], 7: [7], 8: [7], 9: [6]}], 'calpurnia': [1, {2: [3]}], 'angel': [3, {7: [0], 8: [0], 9: [0]}]}\n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}, {'caeser': 1, 'mercy': 1, 'worser': 1}, {'antony': 1, 'caeser': 1, 'mercy': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'angels': 1, 'fools': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}] ['1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "             1   10    2    3    4    5    6    7    8    9\n",
      "antony     1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
      "brutus     1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
      "caeser     1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0\n",
      "cleopatra  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "mercy      1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0\n",
      "worser     1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0\n",
      "fools      0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "fear       0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0\n",
      "in         0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "rush       0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "to         0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "tread      0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "where      0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "calpurnia  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "angels     0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "             1   10    2    3    4    5    6    7    8    9\n",
      "antony     1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
      "brutus     1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
      "caeser     1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0\n",
      "cleopatra  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "mercy      1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0\n",
      "worser     1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0\n",
      "fools      0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "fear       0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0\n",
      "in         0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "rush       0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "to         0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "tread      0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "where      0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "calpurnia  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "angels     0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0\n",
      "            df        idf\n",
      "antony     3.0   3.333333\n",
      "brutus     3.0   3.333333\n",
      "caeser     5.0   2.000000\n",
      "cleopatra  1.0  10.000000\n",
      "mercy      5.0   2.000000\n",
      "worser     4.0   2.500000\n",
      "fools      4.0   2.500000\n",
      "fear       3.0   3.333333\n",
      "in         4.0   2.500000\n",
      "rush       4.0   2.500000\n",
      "to         4.0   2.500000\n",
      "tread      4.0   2.500000\n",
      "where      4.0   2.500000\n",
      "calpurnia  1.0  10.000000\n",
      "angels     3.0   3.333333\n",
      "                   1        10          2    3         4    5         6  \\\n",
      "antony      3.333333  0.000000   3.333333  0.0  0.000000  0.0  3.333333   \n",
      "brutus      3.333333  0.000000   3.333333  0.0  3.333333  0.0  0.000000   \n",
      "caeser      2.000000  0.000000   2.000000  0.0  2.000000  2.0  2.000000   \n",
      "cleopatra  10.000000  0.000000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "mercy       2.000000  0.000000   0.000000  2.0  2.000000  2.0  2.000000   \n",
      "worser      2.500000  0.000000   0.000000  2.5  2.500000  2.5  0.000000   \n",
      "fools       0.000000  2.500000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "fear        0.000000  3.333333   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "in          0.000000  2.500000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "rush        0.000000  2.500000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "to          0.000000  2.500000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "tread       0.000000  2.500000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "where       0.000000  2.500000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "calpurnia   0.000000  0.000000  10.000000  0.0  0.000000  0.0  0.000000   \n",
      "angels      0.000000  0.000000   0.000000  0.0  0.000000  0.0  0.000000   \n",
      "\n",
      "                  7         8         9  \n",
      "antony     0.000000  0.000000  0.000000  \n",
      "brutus     0.000000  0.000000  0.000000  \n",
      "caeser     0.000000  0.000000  0.000000  \n",
      "cleopatra  0.000000  0.000000  0.000000  \n",
      "mercy      0.000000  0.000000  0.000000  \n",
      "worser     0.000000  0.000000  0.000000  \n",
      "fools      2.500000  2.500000  2.500000  \n",
      "fear       3.333333  3.333333  0.000000  \n",
      "in         2.500000  2.500000  2.500000  \n",
      "rush       2.500000  2.500000  2.500000  \n",
      "to         2.500000  2.500000  2.500000  \n",
      "tread      2.500000  2.500000  2.500000  \n",
      "where      2.500000  2.500000  2.500000  \n",
      "calpurnia  0.000000  0.000000  0.000000  \n",
      "angels     3.333333  3.333333  3.333333  \n",
      "1     11.682133\n",
      "10     6.972167\n",
      "2     11.234866\n",
      "3      3.201562\n",
      "4      5.035982\n",
      "5      3.774917\n",
      "6      4.371626\n",
      "7      7.728015\n",
      "8      7.728015\n",
      "9      6.972167\n",
      "dtype: float64\n",
      "                  1        10         2         3         4         5  \\\n",
      "antony     0.285336  0.000000  0.296695  0.000000  0.000000  0.000000   \n",
      "brutus     0.285336  0.000000  0.296695  0.000000  0.661903  0.000000   \n",
      "caeser     0.171202  0.000000  0.178017  0.000000  0.397142  0.529813   \n",
      "cleopatra  0.856008  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "mercy      0.171202  0.000000  0.000000  0.624695  0.397142  0.529813   \n",
      "worser     0.214002  0.000000  0.000000  0.780869  0.496428  0.662266   \n",
      "fools      0.000000  0.358569  0.000000  0.000000  0.000000  0.000000   \n",
      "fear       0.000000  0.478091  0.000000  0.000000  0.000000  0.000000   \n",
      "in         0.000000  0.358569  0.000000  0.000000  0.000000  0.000000   \n",
      "rush       0.000000  0.358569  0.000000  0.000000  0.000000  0.000000   \n",
      "to         0.000000  0.358569  0.000000  0.000000  0.000000  0.000000   \n",
      "tread      0.000000  0.358569  0.000000  0.000000  0.000000  0.000000   \n",
      "where      0.000000  0.358569  0.000000  0.000000  0.000000  0.000000   \n",
      "calpurnia  0.000000  0.000000  0.890086  0.000000  0.000000  0.000000   \n",
      "angels     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "                  6         7         8         9  \n",
      "antony     0.762493  0.000000  0.000000  0.000000  \n",
      "brutus     0.000000  0.000000  0.000000  0.000000  \n",
      "caeser     0.457496  0.000000  0.000000  0.000000  \n",
      "cleopatra  0.000000  0.000000  0.000000  0.000000  \n",
      "mercy      0.457496  0.000000  0.000000  0.000000  \n",
      "worser     0.000000  0.000000  0.000000  0.000000  \n",
      "fools      0.000000  0.323498  0.323498  0.358569  \n",
      "fear       0.000000  0.431331  0.431331  0.000000  \n",
      "in         0.000000  0.323498  0.323498  0.358569  \n",
      "rush       0.000000  0.323498  0.323498  0.358569  \n",
      "to         0.000000  0.323498  0.323498  0.358569  \n",
      "tread      0.000000  0.323498  0.323498  0.358569  \n",
      "where      0.000000  0.323498  0.323498  0.358569  \n",
      "calpurnia  0.000000  0.000000  0.000000  0.000000  \n",
      "angels     0.000000  0.431331  0.431331  0.478091  \n",
      "[{'antony': 1, 'brutus': 1, 'caeser': 1, 'cleopatra': 1, 'mercy': 1, 'worser': 1}, {'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'antony': 1, 'brutus': 1, 'caeser': 1, 'calpurnia': 1}, {'mercy': 1, 'worser': 1}, {'brutus': 1, 'caeser': 1, 'mercy': 1, 'worser': 1}, {'caeser': 1, 'mercy': 1, 'worser': 1}, {'antony': 1, 'caeser': 1, 'mercy': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'angels': 1, 'fools': 1, 'fear': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}, {'angels': 1, 'fools': 1, 'in': 1, 'rush': 1, 'to': 1, 'tread': 1, 'where': 1}]\n",
      "antoni\n",
      "[[0], [], [], [], [], [], [], [], [], []]\n",
      "[[0], [], [0], [], [], [], [], [], [], []]\n",
      "[[0], [], [0], [], [], [], [0], [], [], []]\n",
      "brutu\n",
      "[[0], [], [0], [], [], [], [0], [], [], []]\n",
      "1\n",
      "[0]\n",
      "[[0, 1], [], [0], [], [], [], [0], [], [], []]\n",
      "[[0, 1], [], [0], [], [], [], [0], [], [], []]\n",
      "1\n",
      "[0]\n",
      "[[0, 1], [], [0, 1], [], [], [], [0], [], [], []]\n",
      "[[0, 1], [], [0, 1], [], [0], [], [0], [], [], []]\n",
      "[{'antony': 1, 'brutus': 1}] ['tf-raw']\n",
      "        tf-raw\n",
      "antony       1\n",
      "brutus       1\n",
      "        tf-raw\n",
      "antony     1.0\n",
      "brutus     1.0\n",
      "        df   idf\n",
      "antony   1  10.0\n",
      "brutus   1  10.0\n",
      "        tf-raw\n",
      "antony    10.0\n",
      "brutus    10.0\n",
      "tf-raw    14.142136\n",
      "dtype: float64\n",
      "          tf-raw\n",
      "antony  0.707107\n",
      "brutus  0.707107\n",
      "query length: tf-raw    14.142136\n",
      "dtype: float64\n",
      "        tf-raw  weighted-tf   idf  tf-idf  normalized\n",
      "antony       1          1.0  10.0       1    0.707107\n",
      "brutus       1          1.0  10.0       1    0.707107\n",
      "               1         2\n",
      "antony  0.285336  0.296695\n",
      "brutus  0.285336  0.296695\n",
      "               1         2\n",
      "antony  0.201763  0.209795\n",
      "brutus  0.201763  0.209795\n",
      "2    0.419591\n",
      "1    0.403526\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "positional_index, normalized_doc_df = build_positional_index('docs')\n",
    "\n",
    "q = \"antony brutus\"\n",
    "matched_docs, query_df = put_query(q, positional_index)\n",
    "\n",
    "similarity(query_df, normalized_doc_df, matched_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649318de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
